# Default configuration for DinoV2 LoRA training

# Data configuration
data:
    dataset_path: "/workspace/imagenet_images"
    image_size: 224
    batch_size: 256 #256 for bf16, 128 for fp16
    num_workers: 8
    pin_memory: true

# Model configuration
model:
    name: "facebook/dinov2-small" # Options: facebook/dinov2-small, facebook/dinov2-base, facebook/dinov2-large
    num_classes: 1000
    dropout: 0.1

# LoRA configuration
lora:
    strategy: "fisher"
    r: 4 # LoRA rank
    alpha: 8 # LoRA scaling parameter
    dropout: 0.0 # LoRA dropout
    target_modules: # Modules to apply LoRA to
        - "query"
        - "key"
        - "value"
        - "dense"
    bias: "none" # Bias type for LoRA

fisher_lora:
    ema_decay: 0.99
    update_interval: 1024
    # anneal EMA decay and refresh cadence with cosine schedule
    ema_decay_start: 0.8
    ema_decay_anneal_steps: 4000
    ema_decay_schedule: "cosine"
    update_interval_start: 4
    update_interval_anneal_steps: 4000
    update_interval_schedule: "cosine"
    damping: 0.0001
    min_factor_eig: 0.001 #doesnt affect training, nan safety shit
    freeze_base: true
    train_U: true
    train_V: true
    init_scale: 0.001
    factor_dtype: "float32"
    track_fisher: true
    diagnostics_interval: 1
    whiteness_log_interval: 1 # Set to 1 for per-step pre-refresh logging
    target_modules:
        - "backbone.encoder.layer.*.attention.attention.query"
        - "backbone.encoder.layer.*.attention.attention.key"
        - "backbone.encoder.layer.*.attention.attention.value"
        - "backbone.encoder.layer.*.attention.output.dense"

# Training configuration
training:
    epochs: 4
    learning_rate: 0.0003
    weight_decay: 0.01
    warmup_steps: 1000
    gradient_clip_norm: 0.0
    save_steps: 1000
    eval_steps: 500
    logging_steps: 1

# Optimizer configuration
optimizer:
    type: "adamw" # Options: adamw, adam, sgd
    betas: [0.9, 0.999]
    eps: 0.00000001
    momentum: 0.9
    nesterov: true

# Scheduler configuration
scheduler:
    type: "cosine" # Options: cosine, linear, constant
    warmup_ratio: 0.1

# Logging and saving
logging:
    use_wandb: true
    project_name: "fisher-lora"
    run_name: fisher-rank4-rare-refreshes # Will be auto-generated if null
    log_dir: "./logs"

# Checkpointing
checkpointing:
    save_dir: "./checkpoints"
    save_total_limit: 3
    save_best_only: false
    metric_for_best: "val_accuracy"

# Evaluation
evaluation:
    metrics: ["accuracy", "top5_accuracy"]
    eval_on_start: false

# Mixed precision training
mixed_precision:
    enabled: true
    dtype: "bf16" # Options: fp16, bf16

# Distributed training
distributed:
    enabled: false
    backend: "nccl"

# Reproducibility
seed: 42

# Device
device: "cuda" # Options: auto, cuda, cpu
